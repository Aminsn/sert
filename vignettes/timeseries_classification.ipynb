{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timeseries Classification with the `sert` Python Package\n",
    "\n",
    "**Author:** Amin Shoari Nejad &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Date created:** 2023/09/04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to utilize the sert package for time series classification. The process includes data loading, preprocessing, model instantiation, training, and evaluation.\n",
    "\n",
    "# 1. Imports\n",
    "\n",
    "Assuming that you have installed the sert package, we can import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sert.models import TimeSERT\n",
    "from sert.preprocessing import SeqDataPreparer\n",
    "from sert.datasets import ts_classification\n",
    "from sert.losses import WeightedCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TimeSERT**: This is one of the primary classes of the `sert` package, suitable for both time series classification and multivariate forecasting. In this notebook, we'll use it for time series classification.\n",
    "\n",
    "- **SeqDataPreparer**: This class assists in preparing data for time series classification. Models within the `sert` package expect data in a particular format, and `SeqDataPreparer` facilitates this transformation. It employs the same `fit_transform` and `transform` syntax found in scikit-learn.\n",
    "\n",
    "- **ts_classification**: This function creates a thousand irregular time series of varied lengths. Specifically, both the number of observations and their respective times are randomly determined. 30% of these time series are chosen at random to be anomalous, signifying the presence of an outlier. The function returns the time series data and labels, with \"one\" indicating a time series containing an outlier and \"zero\" otherwise. The objective is to train a model that can discern whether a time series is anomalous. Users can adjust various parameters of the function, like the number of time series or the balance ratio. For comprehensive details, invoke `help(ts_classification)`.\n",
    "\n",
    "- **WeightedCrossentropy**: This is a custom loss function for training models, essentially a weighted iteration of the crossentropy loss function. The weights can be adjusted to balance the loss function. For instance, in situations where data is skewed, the loss function might be weighted to prioritize the underrepresented class. In this notebook, we'll utilize this loss function for training, though we won't assign varied weights to the classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the Dataset\n",
    "\n",
    "Our sert package conveniently provides a time series classification dataset for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>value</th>\n",
       "      <th>var_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.220307</td>\n",
       "      <td>0.886436</td>\n",
       "      <td>var1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6.661911</td>\n",
       "      <td>0.281329</td>\n",
       "      <td>var1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>10.291603</td>\n",
       "      <td>0.532506</td>\n",
       "      <td>var1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>14.312799</td>\n",
       "      <td>0.546110</td>\n",
       "      <td>var1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>16.673076</td>\n",
       "      <td>0.836684</td>\n",
       "      <td>var1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37432</th>\n",
       "      <td>699</td>\n",
       "      <td>85.584778</td>\n",
       "      <td>0.156501</td>\n",
       "      <td>var1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37433</th>\n",
       "      <td>699</td>\n",
       "      <td>89.386548</td>\n",
       "      <td>0.604532</td>\n",
       "      <td>var1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37434</th>\n",
       "      <td>699</td>\n",
       "      <td>92.356314</td>\n",
       "      <td>0.609343</td>\n",
       "      <td>var1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37435</th>\n",
       "      <td>699</td>\n",
       "      <td>94.740727</td>\n",
       "      <td>0.075573</td>\n",
       "      <td>var1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37436</th>\n",
       "      <td>699</td>\n",
       "      <td>96.385273</td>\n",
       "      <td>0.244605</td>\n",
       "      <td>var1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37437 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       time     value var_name\n",
       "0        0   1.220307  0.886436     var1\n",
       "1        0   6.661911  0.281329     var1\n",
       "2        0  10.291603  0.532506     var1\n",
       "3        0  14.312799  0.546110     var1\n",
       "4        0  16.673076  0.836684     var1\n",
       "...    ...        ...       ...      ...\n",
       "37432  699  85.584778  0.156501     var1\n",
       "37433  699  89.386548  0.604532     var1\n",
       "37434  699  92.356314  0.609343     var1\n",
       "37435  699  94.740727  0.075573     var1\n",
       "37436  699  96.385273  0.244605     var1\n",
       "\n",
       "[37437 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = ts_classification()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we'll use the `SeqDataPreparer` to transform the data into a format suitable for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing\n",
    "\n",
    "In order to feed our data into the **TimeSERT** model, we need to perform some preprocessing steps. Specifically, we'll convert the data into a list of numpy arrays suitable for the model. We utilize the `SeqDataPreparer` class for this, which accepts a single argument: `token_capacity`. This represents the maximum number of observations (a.k.a tokens) expected in the training data. \n",
    "\n",
    "After instantiating the `SeqDataPreparer` class, you'll need to:\n",
    "\n",
    "- Call the `fit_transform` method on the training data. This method returns a list of numpy arrays prepared for training.\n",
    "- Call the `transform` method on the test data. This method returns a list of numpy arrays ready for testing. Note that this method assumes that the `fit_transform` method has already been called on the training data and the training and test data are of the same format.\n",
    "\n",
    "The `fit_transform` method requires four arguments:\n",
    "\n",
    "1. **index**: The name of the column that contains the unique identifier for each sequence. In our dataset, this is the `id` column.\n",
    "2. **times**: The name of the column that contains the time variable for each observation. For us, it's the `time` column.\n",
    "3. **names**: The column name that represents the name of the sequence. In this example, it's the `var_name` column. Note that while this example uses a single variable `var1`, you could generally have multiple variables.\n",
    "4. **values**: This refers to the column that contains the values of the variable for each observation. In our example, this is the `value` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the token capacity based on the maximum sequence length in the training set\n",
    "token_cap = X_train.groupby('id').size().max()\n",
    "\n",
    "processor = SeqDataPreparer(token_capacity=token_cap)\n",
    "\n",
    "train_input = processor.fit_transform(X_train,\n",
    "                                      index='id',\n",
    "                                      times='time',\n",
    "                                      names='var_name',\n",
    "                                      values='value')\n",
    "\n",
    "test_input = processor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Instantiation\n",
    "\n",
    "Now, we'll instantiate the TimeSERT model with appropriate hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeSERT(num_var=1,\n",
    "                 emb_dim=15,\n",
    "                 num_head=3,\n",
    "                 ffn_dim=5,\n",
    "                 num_repeat=1,\n",
    "                 num_out=y_train.shape[1],\n",
    "                 task='classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters:\n",
    "\n",
    "- **num_var**: Represents the number of variables in the dataset required by the embedding layer to encode variable names. In this case, there's 1 variable, which is `var1`.\n",
    "- **emb_dim**: Dimension of the embedding layer. Represents the dimension of the latent space.\n",
    "- **num_head**: Number of attention heads.\n",
    "- **ffn_dim**: Dimension of the feedforward layer.\n",
    "- **num_repeat**: Number of times the encoder block is repeated.\n",
    "- **num_out**: Number of output classes.\n",
    "\n",
    "The `emb_dim`, `num_head`, `ffn_dim`, and `num_repeat` hyperparameters determine the size of the model. The larger these values are, the more complex the model becomes. The `num_out` hyperparameter is set to 2, as there are two classes: normal and anomalous. The `task` argument is set to `classification` since the goal is time series classification. For regression, the `task` argument is set to `regression`, which is the default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Compilation\n",
    "\n",
    "The instantiated model is a TensorFlow model. We must compile it using the `compile` method, specifying the optimizer, loss function, and metrics we want to track, as with any TensorFlow model. For this example, we are using the Adam optimizer, the weighted cross-entropy loss function provided by the package, and the accuracy as the metric. Note that the `WeightedCrossentropy` object requires the target variable to be one-hot encoded. Here's what the binary target in this example looks like: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ...\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=WeightedCrossentropy([1, 1]),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this example, we are utilizing the weight vector [1,1] for the weighted cross-entropy loss function. This indicates that both classes are treated with equal importance by the loss function. You can adjust the weights to prioritize one class over the other. For instance, if the data is skewed, you might want to assign a higher weight to the underrepresented class. You can use `compute_class_weight` from scikit-learn to compute the weights and use them in the loss function like below: \n",
    "\n",
    "```python\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=[0,1], y=y_train[:,0]])\n",
    "class_weights = tf.cast(class_weights, dtype=tf.float32)\n",
    "model.compile(optimizer='adam', loss=WeightedCrossentropy(class_weights), metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Training\n",
    "\n",
    "With our data and model ready, we can now train the TimeSERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x12268f670>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_input, y_train, epochs=100, batch_size=250, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Predictions\n",
    "\n",
    "Post-training, we can use the model to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Model Evaluation\n",
    "\n",
    "Lastly, let's evaluate the model's performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 10ms/step - loss: 6.5373e-04 - accuracy: 1.0000\n",
      "Test Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(test_input, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook demonstrated how to effectively use the `sert` package for time series classification. With its intuitive API and preprocessing utilities, working with irregular time series data becomes efficient and straightforward.\n",
    "\n",
    "### Few notes:\n",
    "\n",
    "- In this tutorial, we didn't discuss the hyperparameter tuning process as the goal was to demonstrate how to work with the package API. Nonetheless, the model performed perfectly on the test data with arbitrary hyperparameters.\n",
    "\n",
    "- The package also provides another alternative model to `TimeSERT` called `TimeSERNN` which doesn't use the transformer architecture and only relies on set encoding and feedforward layers. `TimeSERNN` runs much faster but might compromise performance. You can simply replace `TimeSERT` with `TimeSERNN`, which has fewer hyperparameters, like below:\n",
    "\n",
    "```python\n",
    "from sert.models import TimeSERNN\n",
    "\n",
    "model = TimeSERNN(num_var=1,\n",
    "                 emb_dim=15,\n",
    "                 num_out=y_train.shape[1],\n",
    "                 task='classification')\n",
    "                 \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virt_Sert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
