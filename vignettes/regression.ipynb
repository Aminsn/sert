{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modelling with the `sert` Python Package\n",
    "\n",
    "**Author:** Amin Shoari Nejad &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Date created:** 2023/09/04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to utilize the sert package for predicting a continuous outcome. The process includes data loading, preprocessing, model instantiation, training, and evaluation.\n",
    "\n",
    "# 1. Imports\n",
    "\n",
    "Assuming that you have installed the sert package, we can import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sert.models import SERT\n",
    "from sert.preprocessing import DataPreparer\n",
    "from sert.losses import MaskedMSE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **SERT**: This is one of the primary classes of the `sert` package, suitable for both classification and regression predictive modelling. In this notebook, we'll use it for a regression problem.\n",
    "\n",
    "- **DataPreparer**: This class assists in preparing data. Models within the `sert` package expect data in a particular format, and `DataPreparer` facilitates this transformation. It employs the same `fit_transform` and `transform` syntax found in scikit-learn.\n",
    "\n",
    "- **MaskedMSE**: This is a custom loss function for training models on sparse data. It is a masked version of the mean squared error (MSE) loss function that masks out the missing values in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the Dataset\n",
    "\n",
    "In this tutorial we will use the Ames housing dataset which is a well-known dataset including both numerical and categorical features. Our goal is to predict the sales price of the houses in the dataset using their features in the dataset. To learn more about the dataset please refer to [this link](https://www.kaggle.com/datasets/shashanknecrothapa/ames-housing-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 80 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   MSSubClass     1460 non-null   int64  \n",
      " 1   MSZoning       1460 non-null   object \n",
      " 2   LotFrontage    1460 non-null   float64\n",
      " 3   LotArea        1460 non-null   int64  \n",
      " 4   Street         1460 non-null   object \n",
      " 5   Alley          1460 non-null   object \n",
      " 6   LotShape       1460 non-null   object \n",
      " 7   LandContour    1460 non-null   object \n",
      " 8   Utilities      1460 non-null   object \n",
      " 9   LotConfig      1460 non-null   object \n",
      " 10  LandSlope      1460 non-null   object \n",
      " 11  Neighborhood   1460 non-null   object \n",
      " 12  Condition1     1460 non-null   object \n",
      " 13  Condition2     1460 non-null   object \n",
      " 14  BldgType       1460 non-null   object \n",
      " 15  HouseStyle     1460 non-null   object \n",
      " 16  OverallQual    1460 non-null   int64  \n",
      " 17  OverallCond    1460 non-null   int64  \n",
      " 18  YearBuilt      1460 non-null   int64  \n",
      " 19  YearRemodAdd   1460 non-null   int64  \n",
      " 20  RoofStyle      1460 non-null   object \n",
      " 21  RoofMatl       1460 non-null   object \n",
      " 22  Exterior1st    1460 non-null   object \n",
      " 23  Exterior2nd    1460 non-null   object \n",
      " 24  MasVnrType     588 non-null    object \n",
      " 25  MasVnrArea     1460 non-null   float64\n",
      " 26  ExterQual      1460 non-null   object \n",
      " 27  ExterCond      1460 non-null   object \n",
      " 28  Foundation     1460 non-null   object \n",
      " 29  BsmtQual       1460 non-null   object \n",
      " 30  BsmtCond       1460 non-null   object \n",
      " 31  BsmtExposure   1460 non-null   object \n",
      " 32  BsmtFinType1   1460 non-null   object \n",
      " 33  BsmtFinSF1     1460 non-null   int64  \n",
      " 34  BsmtFinType2   1460 non-null   object \n",
      " 35  BsmtFinSF2     1460 non-null   int64  \n",
      " 36  BsmtUnfSF      1460 non-null   int64  \n",
      " 37  TotalBsmtSF    1460 non-null   int64  \n",
      " 38  Heating        1460 non-null   object \n",
      " 39  HeatingQC      1460 non-null   object \n",
      " 40  CentralAir     1460 non-null   object \n",
      " 41  Electrical     1460 non-null   object \n",
      " 42  1stFlrSF       1460 non-null   int64  \n",
      " 43  2ndFlrSF       1460 non-null   int64  \n",
      " 44  LowQualFinSF   1460 non-null   int64  \n",
      " 45  GrLivArea      1460 non-null   int64  \n",
      " 46  BsmtFullBath   1460 non-null   int64  \n",
      " 47  BsmtHalfBath   1460 non-null   int64  \n",
      " 48  FullBath       1460 non-null   int64  \n",
      " 49  HalfBath       1460 non-null   int64  \n",
      " 50  BedroomAbvGr   1460 non-null   int64  \n",
      " 51  KitchenAbvGr   1460 non-null   int64  \n",
      " 52  KitchenQual    1460 non-null   object \n",
      " 53  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 54  Functional     1460 non-null   object \n",
      " 55  Fireplaces     1460 non-null   int64  \n",
      " 56  FireplaceQu    1460 non-null   object \n",
      " 57  GarageType     1460 non-null   object \n",
      " 58  GarageYrBlt    1460 non-null   float64\n",
      " 59  GarageFinish   1460 non-null   object \n",
      " 60  GarageCars     1460 non-null   int64  \n",
      " 61  GarageArea     1460 non-null   int64  \n",
      " 62  GarageQual     1460 non-null   object \n",
      " 63  GarageCond     1460 non-null   object \n",
      " 64  PavedDrive     1460 non-null   object \n",
      " 65  WoodDeckSF     1460 non-null   int64  \n",
      " 66  OpenPorchSF    1460 non-null   int64  \n",
      " 67  EnclosedPorch  1460 non-null   int64  \n",
      " 68  3SsnPorch      1460 non-null   int64  \n",
      " 69  ScreenPorch    1460 non-null   int64  \n",
      " 70  PoolArea       1460 non-null   int64  \n",
      " 71  PoolQC         1460 non-null   object \n",
      " 72  Fence          1460 non-null   object \n",
      " 73  MiscFeature    1460 non-null   object \n",
      " 74  MiscVal        1460 non-null   int64  \n",
      " 75  MoSold         1460 non-null   int64  \n",
      " 76  YrSold         1460 non-null   int64  \n",
      " 77  SaleType       1460 non-null   object \n",
      " 78  SaleCondition  1460 non-null   object \n",
      " 79  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(34), object(43)\n",
      "memory usage: 912.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "url = \"https://raw.githubusercontent.com/INRIA/scikit-learn-mooc/main/datasets/ames_housing_no_missing.csv\"\n",
    "\n",
    "ames_data = pd.read_csv(url)\n",
    "ames_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is complete with no missing values. However, to make the prediction task more challenging we introduce missing values in the dataset by randomly replacing 10% of the feature values with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating the target from input variables\n",
    "X = ames_data.drop(['SalePrice'], axis=1)\n",
    "# randomly replace 10% of the data with NaN\n",
    "X = X.mask(np.random.random(X.shape) < .1)\n",
    "# choose the target variable\n",
    "y = ames_data[['SalePrice']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing\n",
    "\n",
    "In order to feed our data into the **SERT** model, we need to perform some preprocessing steps.\n",
    "First we need to scale the data. We use the `StandardScaler` from scikit-learn for this purpose. To do so we need to fit the scaler on the training data and then transform both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# IMPORTANT STEP: reseting the index to avoid problems with the shuffling of the data in the subsequent steps\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Standardising numerical predictors\n",
    "# Identify numerical columns in the training set\n",
    "numerical_cols = X_train.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "# Instantiate the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the numerical columns of the training data and transform them\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "\n",
    "# Use the fitted scaler to transform the numerical columns of the test data\n",
    "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models in sert are designed to work with data in set format (i.e., each row is a single observation with variable name, value). We melt the data into this format as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MSSubClass</td>\n",
       "      <td>-0.867671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MSSubClass</td>\n",
       "      <td>0.076689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MSSubClass</td>\n",
       "      <td>-0.631581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MSSubClass</td>\n",
       "      <td>-0.159401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MSSubClass</td>\n",
       "      <td>-0.159401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92267</th>\n",
       "      <td>1163</td>\n",
       "      <td>SaleCondition</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92268</th>\n",
       "      <td>1164</td>\n",
       "      <td>SaleCondition</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92269</th>\n",
       "      <td>1165</td>\n",
       "      <td>SaleCondition</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92270</th>\n",
       "      <td>1166</td>\n",
       "      <td>SaleCondition</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92271</th>\n",
       "      <td>1167</td>\n",
       "      <td>SaleCondition</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92272 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index       variable     value\n",
       "0          0     MSSubClass -0.867671\n",
       "1          1     MSSubClass  0.076689\n",
       "2          2     MSSubClass -0.631581\n",
       "3          3     MSSubClass -0.159401\n",
       "4          4     MSSubClass -0.159401\n",
       "...      ...            ...       ...\n",
       "92267   1163  SaleCondition    Normal\n",
       "92268   1164  SaleCondition    Normal\n",
       "92269   1165  SaleCondition    Normal\n",
       "92270   1166  SaleCondition    Normal\n",
       "92271   1167  SaleCondition    Normal\n",
       "\n",
       "[92272 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_long = X_train.reset_index().melt(\n",
    "    id_vars=['index'], value_vars=X.columns)\n",
    "X_test_long = X_test.reset_index().melt(\n",
    "    id_vars=['index'], value_vars=X.columns)\n",
    "X_train_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll convert the input data into a list of numpy arrays suitable for the model. We utilize the `DataPreparer` class for this, which accepts a single argument: `token_capacity`. This represents the maximum number of observations (a.k.a tokens) expected in the training data. \n",
    "\n",
    "After instantiating the `DataPreparer` class, you'll need to:\n",
    "\n",
    "- Call the `fit_transform` method on the training data. This method returns a list of numpy arrays prepared for training.\n",
    "- Call the `transform` method on the test data. This method returns a list of numpy arrays ready for testing. Note that this method assumes that the `fit_transform` method has already been called on the training data and the training and test data are of the same format.\n",
    "\n",
    "The `fit_transform` method requires four arguments:\n",
    "\n",
    "1. **index**: The name of the column that contains the unique identifier for each sequence. In our dataset, this is the `index` column.\n",
    "3. **names**: The column name that represents the name of the variables. In this example, it's the `variable` column.\n",
    "4. **values**: This refers to the column containing the values of the variable for each observation. In our example, it's the `value` column. It's worth noting that in this particular example this column has both numerical and categorical values. This is not an issue, as `DataPreparer` appends the character values to the variable names, encoding them together in the model, while masking their values in the value column. Alternatively, you can also use one-hot encoding for the categorical variables in the dataset before doing the subsequent steps. This is not necessary, but it may improve the performance of the model in some applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the token capacity based on the maximum input length in the training set\n",
    "token_cap = X_train_long.groupby('index').size().max()\n",
    "\n",
    "processor = DataPreparer(token_capacity=token_cap)\n",
    "\n",
    "train_input = processor.fit_transform(X_train_long,\n",
    "                                      index='index',\n",
    "                                      names='variable',\n",
    "                                      values='value')\n",
    "\n",
    "test_input = processor.transform(X_test_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input data is now ready to be fed into the model. We also need to prepare the target data. Since we're using a masked MSE loss function, we need to create a mask for the target data. We also need to stack the target data and the mask together to create a single numpy array that will be used by the loss function (`MaskedMSE` requires the target and the mask to be stacked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the output mask: 1 if the value is not missing, 0 otherwise\n",
    "y_mask = ~np.isnan(y_train.values)\n",
    "# impute the missing values with 0, will be masked out later and doesn't affect the loss\n",
    "y_train = np.nan_to_num(y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to stack the target data and the mask together to create a single numpy array that will be used by the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = np.stack([y_train, y_mask], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Instantiation\n",
    "\n",
    "Now, we'll instantiate the SERT model with appropriate hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_var = len(processor.name_to_int)\n",
    "\n",
    "model = SERT(num_var=num_var,\n",
    "             emb_dim=15,\n",
    "             num_head=3,\n",
    "             ffn_dim=5,\n",
    "             num_repeat=1,\n",
    "             num_out=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters:\n",
    "\n",
    "- **num_var**: Represents the number of variables in the dataset required by the embedding layer to encode variable ids. In our example, `processor` has already encoded the variable names into ids and stored the mapping in the `name_to_int` attribute. We can use this attribute to determine the number of variables in the dataset. Note that this is not necessarily equal to the number of columns minus the target in the original dataset, since the categorical values are merged with the variable names to create new combinations that need to be embedded.\n",
    "- **emb_dim**: Dimension of the embedding layer. Represents the dimension of the latent space.\n",
    "- **num_head**: Number of attention heads.\n",
    "- **ffn_dim**: Dimension of the feedforward layer.\n",
    "- **num_repeat**: Number of times the encoder block is repeated.\n",
    "- **num_out**: Dimension of the target variable. In this case, we are only predicting a single variable, so this is equal to one. But if we were predicting multiple variables, this would be equal to the number of target variables.\n",
    "\n",
    "The `emb_dim`, `num_head`, `ffn_dim`, and `num_repeat` hyperparameters determine the size of the model. The larger these values are, the more complex the model becomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Compilation\n",
    "\n",
    "The instantiated model is a TensorFlow model. We must compile it using the `compile` method, specifying the optimizer, loss function, and metrics we want to track, as with any TensorFlow model. For this example, we are using the Adam optimizer, the masked MSE loss function provided by the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=MaskedMSE(), optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Training\n",
    "\n",
    "With our data and model ready, we can now train the SERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 40ms/step - loss: 307517312.0000\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 307068000.0000\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 306655072.0000\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 306260192.0000\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 306375072.0000\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 305321376.0000\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 305454656.0000\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 304880192.0000\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 304810560.0000\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 304759360.0000\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 305139488.0000\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 305066144.0000\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 305756512.0000\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 305896768.0000\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 304943776.0000\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 305121856.0000\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 304547680.0000\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 304251360.0000\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 304504544.0000\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 304449248.0000\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 304177696.0000\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 303393856.0000\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 303193440.0000\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 303393984.0000\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 303362400.0000\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 305821888.0000\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 308003552.0000\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 312837376.0000\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 314251360.0000\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 307712896.0000\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 305003680.0000\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 305747520.0000\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 304220416.0000\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 304311520.0000\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 302724928.0000\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 302715392.0000\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 302734560.0000\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 303908800.0000\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 302409376.0000\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 301493024.0000\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 34ms/step - loss: 300991488.0000\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 301392160.0000\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 300184576.0000\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 299691040.0000\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 300232864.0000\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 301649760.0000\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 304057184.0000\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 304716896.0000\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 305656512.0000\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 304246304.0000\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 304172480.0000\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 302884992.0000\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 301347072.0000\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 300878432.0000\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 299373216.0000\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 299306560.0000\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 298742368.0000\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 298210528.0000\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 297624160.0000\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 297013024.0000\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 297593760.0000\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 298264416.0000\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 300107072.0000\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 298913760.0000\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 298276672.0000\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 298135232.0000\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 298458464.0000\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 298456896.0000\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 297469920.0000\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 296579072.0000\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 296020288.0000\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 296482944.0000\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 297337184.0000\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 297478048.0000\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 297351936.0000\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 296923648.0000\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 295517664.0000\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 294913440.0000\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 294694976.0000\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 294834688.0000\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 294536192.0000\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 294054944.0000\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 295173728.0000\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 294504896.0000\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 293946368.0000\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 293754400.0000\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 293360576.0000\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 293762528.0000\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 293014912.0000\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 292437056.0000\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 292410944.0000\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 292304192.0000\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 292384832.0000\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 292271168.0000\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 292212448.0000\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 291685536.0000\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 291524224.0000\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 290890336.0000\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 291005984.0000\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 290590304.0000\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 291177632.0000\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 291380896.0000\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 292489568.0000\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 295379712.0000\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 295608896.0000\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 294554144.0000\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 292889152.0000\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 293051104.0000\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 293213088.0000\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 291739648.0000\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 292845984.0000\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 294400576.0000\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 293015744.0000\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 291643104.0000\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 290915776.0000\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 291957024.0000\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 291370368.0000\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 291290784.0000\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 290121376.0000\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 288912192.0000\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 288780064.0000\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 288266240.0000\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 287698656.0000\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 287675264.0000\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 287122976.0000\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 287412416.0000\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 287252192.0000\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 286406656.0000\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 287115520.0000\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 286734432.0000\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 288486944.0000\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 295190464.0000\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 296236480.0000\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 293158912.0000\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 293396480.0000\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 291704448.0000\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 289368800.0000\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 285921568.0000\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 285078016.0000\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 285013216.0000\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 285725728.0000\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 285411456.0000\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 285527776.0000\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 284497440.0000\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 283941120.0000\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 283320416.0000\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 282917056.0000\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 282759744.0000\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 282679040.0000\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 283218752.0000\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 282852800.0000\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 283106944.0000\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 282658016.0000\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 282804960.0000\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 37ms/step - loss: 282401760.0000\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 283307680.0000\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 282997312.0000\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 283397472.0000\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 284472608.0000\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 282563872.0000\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 283280384.0000\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 282417408.0000\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 282434208.0000\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 284584160.0000\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 283216224.0000\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 285182624.0000\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 287043008.0000\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 284082848.0000\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 282994144.0000\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 281747616.0000\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 281672704.0000\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 280663936.0000\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 280173408.0000\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 279842144.0000\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 280114176.0000\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 280702432.0000\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 281484480.0000\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 281137280.0000\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 280970880.0000\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 281168448.0000\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 280302976.0000\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 280102688.0000\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 279916896.0000\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 279326720.0000\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 279408288.0000\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 278953728.0000\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 279272192.0000\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 278739648.0000\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 277884800.0000\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 277680352.0000\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 277646304.0000\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 278066720.0000\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 277612800.0000\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 33ms/step - loss: 277049184.0000\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 38ms/step - loss: 277117568.0000\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 40ms/step - loss: 277300448.0000\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 277510272.0000\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 32ms/step - loss: 278659456.0000\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 277220352.0000\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 277560032.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2b09bdd60>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_input, train_output, epochs=200, batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 3ms/step\n",
      "RMSE: 41267.59\n",
      "R2: 0.78\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred = model.predict(test_input)\n",
    "y_pred = y_pred.reshape(-1)\n",
    "\n",
    "test_obs = y_test.to_numpy().reshape(-1)\n",
    "\n",
    "# # Calculate performance metrics\n",
    "rmse = np.sqrt(mean_squared_error(test_obs, y_pred))\n",
    "r2 = r2_score(test_obs, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook demonstrated how to effectively use the `sert` package for regression problems. With its intuitive API and preprocessing utilities, working with tabular data with missing values becomes efficient and straightforward.\n",
    "\n",
    "### Few notes:\n",
    "\n",
    "- In this tutorial, we didn't discuss the hyperparameter tuning process as the goal was to demonstrate how to work with the package API. We encourage you to experiment with different hyperparameters and see how they affect the model performance.\n",
    "\n",
    "- We showed that `SERT` can handle categorical variables in the dataset without the need for one-hot encoding. \n",
    "\n",
    "- We also showed that `SERT` can handle missing values naturally in the dataset without the need for imputation. However, it's worth noting that the model's performance might be affected by the percentage of missing values in the dataset. In this tutorial, we introduced 10% missing values completely at random into the dataset. You can experiment with different percentages and other missing value mechanisms, such as MNAR, to see how they impact the model's performance.\n",
    "\n",
    "- The package also provides another alternative model to `SERT` called `SERNN` which doesn't use the transformer architecture and only relies on set encoding and feedforward layers. `SERNN` runs much faster but might compromise performance. You can simply replace `SERT` with `SERNN`, which has fewer hyperparameters, like below:\n",
    "\n",
    "```python\n",
    "from sert.models import SERNN\n",
    "\n",
    "model = SERNN(num_var=1,\n",
    "              emb_dim=15,\n",
    "              num_out=y_train.shape[1],\n",
    "              task='regression')\n",
    "                 \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virt_Sert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
