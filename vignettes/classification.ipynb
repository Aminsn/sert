{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with the `sert` Python Package\n",
    "\n",
    "**Author:** Amin Shoari Nejad &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Date created:** 2023/09/04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to utilize the sert package for predicting a continuous outcome. The process includes data loading, preprocessing, model instantiation, training, and evaluation.\n",
    "\n",
    "# 1. Imports\n",
    "\n",
    "Assuming that you have installed the sert package, we can import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sert.models import SERT\n",
    "from sert.preprocessing import DataPreparer\n",
    "from sert.losses import WeightedCrossentropy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **SERT**: This is one of the primary classes of the `sert` package, suitable for both classification and regression predictive modelling. In this notebook, we'll use it for a classification problem.\n",
    "\n",
    "- **DataPreparer**: This class assists in preparing data. Models within the `sert` package expect data in a particular format, and `DataPreparer` facilitates this transformation. It employs the same `fit_transform` and `transform` syntax found in scikit-learn.\n",
    "\n",
    "- **MaskedMSE**: This is a custom loss function for training models on sparse data. It is a masked version of the mean squared error (MSE) loss function that masks out the missing values in the output.\n",
    "\n",
    "- **WeightedCrossentropy**: This is a custom loss function for training models, essentially a weighted iteration of the crossentropy loss function. The weights can be adjusted to balance the loss function. For instance, in situations where data is skewed, the loss function might be weighted to prioritize the underrepresented class. In this notebook, we'll utilize this loss function for training, though we won't assign varied weights to the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the Dataset\n",
    "\n",
    "In this tutorial we will use the well-known [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris). The dataset contains 150 samples, each with four features and one of three possible classes. The goal is to predict the class of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X = data.data\n",
    "# randomly replace 10% of the data with NaN\n",
    "np.random.seed(1)\n",
    "X[np.random.random(X.shape) < .05] = np.nan\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing\n",
    "\n",
    "In order to feed our data into the **SERT** model, we need to perform some preprocessing steps.\n",
    "First we need to scale the data. We use the `StandardScaler` from scikit-learn for this purpose. To do so we need to fit the scaler on the training data and then transform both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "\n",
    "features = data.feature_names\n",
    "X_test.columns = features\n",
    "X_train.columns = features\n",
    "\n",
    "# Scaling the data\n",
    "# Instantiate the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the numerical columns of the training data and transform them\n",
    "X_train[features] = scaler.fit_transform(X_train[features])\n",
    "\n",
    "# Use the fitted scaler to transform the numerical columns of the test data\n",
    "X_test[features] = scaler.transform(X_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models in sert are designed to work with data in set format (i.e., each row is a single observation with variable name, value). We melt the data into this format as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>-1.468028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>0</td>\n",
       "      <td>petal width (cm)</td>\n",
       "      <td>-1.289006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0</td>\n",
       "      <td>sepal width (cm)</td>\n",
       "      <td>1.263388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0</td>\n",
       "      <td>petal length (cm)</td>\n",
       "      <td>-1.545837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>-0.134894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>118</td>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>-0.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>119</td>\n",
       "      <td>sepal width (cm)</td>\n",
       "      <td>-0.142236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>119</td>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>1.561822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>119</td>\n",
       "      <td>petal length (cm)</td>\n",
       "      <td>1.269136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>119</td>\n",
       "      <td>petal width (cm)</td>\n",
       "      <td>1.244558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index           variable     value\n",
       "0        0  sepal length (cm) -1.468028\n",
       "360      0   petal width (cm) -1.289006\n",
       "120      0   sepal width (cm)  1.263388\n",
       "240      0  petal length (cm) -1.545837\n",
       "1        1  sepal length (cm) -0.134894\n",
       "..     ...                ...       ...\n",
       "118    118  sepal length (cm) -0.013700\n",
       "239    119   sepal width (cm) -0.142236\n",
       "119    119  sepal length (cm)  1.561822\n",
       "359    119  petal length (cm)  1.269136\n",
       "479    119   petal width (cm)  1.244558\n",
       "\n",
       "[480 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_long = X_train.reset_index().melt(\n",
    "    id_vars=['index'], value_vars=features)\n",
    "X_test_long = X_test.reset_index().melt(\n",
    "    id_vars=['index'], value_vars=features)\n",
    "\n",
    "X_train_long.sort_values(by=['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll convert the input data into a list of numpy arrays suitable for the model. We utilize the `DataPreparer` class for this, which accepts a single argument: `token_capacity`. This represents the maximum number of observations (a.k.a tokens) expected in the training data. \n",
    "\n",
    "After instantiating the `DataPreparer` class, you'll need to:\n",
    "\n",
    "- Call the `fit_transform` method on the training data. This method returns a list of numpy arrays prepared for training.\n",
    "- Call the `transform` method on the test data. This method returns a list of numpy arrays ready for testing. Note that this method assumes that the `fit_transform` method has already been called on the training data and the training and test data are of the same format.\n",
    "\n",
    "The `fit_transform` method requires four arguments:\n",
    "\n",
    "1. **index**: The name of the column that contains the unique identifier for each sequence. In our dataset, this is the `index` column.\n",
    "3. **names**: The column name that represents the name of the variables. In this example, it's the `variable` column.\n",
    "4. **values**: This refers to the column containing the values of the variable for each observation. In our example, it's the `value` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the token capacity based on the maximum input length in the training set\n",
    "token_cap = X_train_long.groupby('index').size().max()\n",
    "\n",
    "processor = DataPreparer(token_capacity=token_cap)\n",
    "\n",
    "train_input = processor.fit_transform(X_train_long,\n",
    "                                      index='index',\n",
    "                                      names='variable',\n",
    "                                      values='value')\n",
    "\n",
    "test_input = processor.transform(X_test_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input data is now ready to be fed into the model. We also need to prepare the target data. Since we want to use the `WeightedCrossentropy` class, we need to one-hot encode the target data that is required for this loss function. We can do this using the `OneHotEncoder` class from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)\n",
    "train_output = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "test_output = encoder.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Instantiation\n",
    "\n",
    "Now, we'll instantiate the SERT model with appropriate hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SERT(num_var=4,\n",
    "             emb_dim=15,\n",
    "             num_head=3,\n",
    "             ffn_dim=5,\n",
    "             num_repeat=1,\n",
    "             num_out=3,\n",
    "             task='classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters:\n",
    "\n",
    "- **num_var**: Represents the number of variables in the dataset required by the embedding layer to encode variable names. \n",
    "- **emb_dim**: Dimension of the embedding layer. Represents the dimension of the latent space.\n",
    "- **num_head**: Number of attention heads.\n",
    "- **ffn_dim**: Dimension of the feedforward layer.\n",
    "- **num_repeat**: Number of times the encoder block is repeated.\n",
    "- **num_out**: Number of output classes.\n",
    "\n",
    "The `emb_dim`, `num_head`, `ffn_dim`, and `num_repeat` hyperparameters determine the size of the model. The larger these values are, the more complex the model becomes. The `num_out` hyperparameter is set to 3, as there are three classes. The `task` argument is set to `classification` since the goal is to classify the species. For regression, the `task` argument is set to `regression`, which is the default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Compilation\n",
    "\n",
    "The instantiated model is a TensorFlow model. We must compile it using the `compile` method, specifying the optimizer, loss function, and metrics we want to track, as with any TensorFlow model. For this example, we are using the Adam optimizer, the weighted cross-entropy loss function provided by the package, and the accuracy as the metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=WeightedCrossentropy([1, 1, 1]),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this example, we are utilizing the weight vector [1,1,1] for the weighted cross-entropy loss function. This indicates that all classes are treated with equal importance by the loss function. You can adjust the weights to prioritize one class over the others. For instance, if the data is skewed, you might want to assign a higher weight to the underrepresented class. You can use `compute_class_weight` from scikit-learn to compute the weights and use them in the loss function like below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# import tensorflow as tf\n",
    "\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=[0,1,2], y=y_train)\n",
    "# class_weights = tf.cast(class_weights, dtype=tf.float32)\n",
    "# model.compile(optimizer='adam', loss=WeightedCrossentropy(class_weights), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Training\n",
    "\n",
    "With our data and model ready, we can now train the SERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x13aed5cf0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_input, train_output, epochs=100, batch_size=75, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 419ms/step - loss: 0.0557 - accuracy: 1.0000\n",
      "Test Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "_loss_, accuracy = model.evaluate(test_input, test_output)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook demonstrated how to effectively use the `sert` package for classification. With its intuitive API and preprocessing utilities, working with tabular data with missing values becomes efficient and straightforward.\n",
    "\n",
    "### Few notes:\n",
    "\n",
    "- In this tutorial, we didn't discuss the hyperparameter tuning process as the goal was to demonstrate how to work with the package API. Nonetheless, the model performed perfectly on the test data with arbitrary hyperparameters.\n",
    "\n",
    "- We showed that with the right informative features, the model can achieve high performance even on small datasets with missing values. \n",
    "\n",
    "- The package also provides another alternative model to `SERT` called `SERNN` which doesn't use the transformer architecture and only relies on set encoding and feedforward layers. `SERNN` runs much faster but might compromise performance. You can simply replace `SERT` with `SERNN`, which has fewer hyperparameters, like below:\n",
    "\n",
    "```python\n",
    "from sert.models import SERNN\n",
    "\n",
    "model = SERNN(num_var=1,\n",
    "              emb_dim=15,\n",
    "              num_out=y_train.shape[1],\n",
    "              task='classification')\n",
    "                 \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virt_Sert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
